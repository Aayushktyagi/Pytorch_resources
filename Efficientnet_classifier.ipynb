{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Efficientnet_classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN87PJyEBS1uEfCzy0FlBe2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aayushktyagi/Pytorch_resources/blob/master/Efficientnet_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpxTSIfIlz-0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FphVxiXD-qcl",
        "colab_type": "text"
      },
      "source": [
        "# Stanford Dog classification \n",
        "\n",
        "*   Loading dataset \n",
        "  1.   wget images\n",
        "  2.   unzip\n",
        "  3.   resize to 256x256\n",
        "\n",
        "\n",
        "*   Dataset Visualization\n",
        "*   Initialize EfficientNet\n",
        "*   Train EfficientNet\n",
        "*   Evaluate model \n",
        "*   Visualize predictions \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLlPviRDA2Ck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from os import path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8Kirjbmo32r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load dataset\n",
        "if os.path.exists(\"images.tar\") and os.path.exists(\"annotation.tar\"):\n",
        "  # unzip images and annotations \n",
        "  print(\"file exist: Unzipping ....\")\n",
        "  !tar -xvf images.tar\n",
        "  !tar -xvf annotation.tar\n",
        "else:\n",
        "  !wget http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar\n",
        "  !wget http://vision.stanford.edu/aditya86/ImageNetDogs/annotation.tar\n",
        "  # unzip images and annotations \n",
        "  !tar -xvf images.tar\n",
        "  !tar -xvf annotation.tar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBgbiHjYErii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch \n",
        "import numpy as np\n",
        "from torchvision import models, datasets , transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxKHXE6EJyLj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9d650b4c-e7b5-4f19-8fd8-e2018acc3e84"
      },
      "source": [
        "\n",
        "\n",
        "# folder_name = 'Images'\n",
        "\n",
        "# images_datasets = {x: datasets.ImageFolder(folder_name,\n",
        "#                                            data_transforms[x])\n",
        "#                   for x in ['Train','Test']} \n",
        "# dataloader = {x: torch.utils.data.DataLoader(images_datasets[x],batch_size = 8,\n",
        "#                                              shuffle = True , num_workers = 4)\n",
        "#                   for x in ['Train', 'Test']}\n",
        "\n",
        "# dataset_size = {x: len(images_datasets[x]) for x in ['Train','Test']}\n",
        "# print(dataset_size)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Train': 20580, 'Test': 20580}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7R7_keyERwes",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pre-process datasest\n",
        "\n",
        "data_transforms = {\n",
        "    'Train': transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.RandomRotation(45),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 406],[0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'Test': transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "}\n",
        "\n",
        "class CustomDataSet(Dataset):\n",
        "  def __init__(self, dataset, transform = None):\n",
        "    self.dataset = dataset\n",
        "    self.transform = transform\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    if self.transform:\n",
        "      x = self.transform(self.dataset[index][0])\n",
        "    else:\n",
        "      x = self.dataset[index][0]\n",
        "    y = self.dataset[index][1]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "\n",
        "#get data instance\n",
        "imagedatafolder = datasets.ImageFolder(folder_name)\n",
        "traindataset = CustomDataSet(imagedatafolder , data_transform['Train'])\n",
        "validdataset = CustomDataSet(imagedatafolder , data_transform['Test'])\n",
        "testdataset = CustomDataSet(imagedatafolder, data_transform['Test'])\n",
        "\n",
        "# train valid test split\n",
        "\n",
        "train_size = 0.8\n",
        "data_shuffle = True\n",
        "random_seed = 42\n",
        "batch_size = 32\n",
        "num_worker = 4\n",
        "\n",
        "dataset_size = len(imagedatafolder)\n",
        "indices = list(range(dataset_size))\n",
        "\n",
        "split = int(np.floor(train_size * dataset_size))\n",
        "split2 = int(np.floor(train_size + ((1-train_size) / 2)) * dataset_size)\n",
        "\n",
        "if data_shuffle:\n",
        "  np.random.seed(random_seed)\n",
        "  np.random.shuffle(indices)\n",
        "\n",
        "train_idx , valid_idx , test_idx = indices[:split] , indices[split:split1] , indices[split2:]\n",
        "\n",
        "trainsplit = Subset(traindataset , train_idx)\n",
        "validsplit = Subset(validdataset , valid_idx)\n",
        "testsplit = Subset(testdataset , test_idx)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainsplit , batch_size = batch_size,\n",
        "                                          num_worker = num_worker , drop_last = True)\n",
        "validloader = torch.utils.data.DataLoader(validsplit, batch_size = batch_size,\n",
        "                                          num-worker = num_worker , drop_last= True)\n",
        "testloader = torch.utils.data.DataLoader(testsplit, batch_size = batch_size,\n",
        "                                         num_worker= num_worker, drop_last= True)\n",
        "\n",
        "# check size of train, valid, test\n",
        "\n",
        "print(\"Train:{} , Valid:{} , Test:{}\".format(len(trainloader),len(validloader),len(testloader)))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}